#!/usr/bin/env python3
"""
æ–°è API æ‡‰ç”¨ - ä½¿ç”¨åˆ†é›¢çš„æ’ç¨‹å™¨
"""

from fastapi import FastAPI, HTTPException, Query, Depends, BackgroundTasks
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import logging
import os
from contextlib import asynccontextmanager
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel
from dotenv import load_dotenv

# åŒ¯å…¥åˆ†é›¢çš„æ’ç¨‹å™¨
from .scheduler import start_scheduler, stop_scheduler, get_scheduler_status, get_scheduler_interval, run_scraper_job

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­å®šæ—¥èªŒ
from logging.handlers import TimedRotatingFileHandler
from pathlib import Path

LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

logger = logging.getLogger("news_api")
logger.setLevel(logging.INFO)

# æ¸…é™¤ç¾æœ‰çš„ handlers
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# è¨­å®šæ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ™‚é–“è¼ªè½‰æ–‡ä»¶è™•ç†å™¨
file_handler = TimedRotatingFileHandler(
    filename=LOG_DIR / "api.log",
    when='midnight',
    interval=1,
    backupCount=7,
    encoding='utf-8'
)
file_handler.setFormatter(formatter)
file_handler.suffix = "%Y%m%d"

# æ§åˆ¶å°è™•ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

# æ·»åŠ è™•ç†å™¨
logger.addHandler(file_handler)
logger.addHandler(console_handler)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å•Ÿå‹•æ™‚
    logger.info("ğŸš€ æ–°è API æœå‹™å•Ÿå‹•")
    
    # å•Ÿå‹•æ’ç¨‹å™¨
    interval = get_scheduler_interval()
    await start_scheduler(interval)
    logger.info(f"ğŸ“… æ’ç¨‹å™¨å·²å•Ÿå‹• - æ¯ {interval} å°æ™‚åŸ·è¡Œçˆ¬èŸ²")
    
    yield
    
    # é—œé–‰æ™‚
    logger.info("ğŸ‘‹ æ­£åœ¨é—œé–‰æ–°è API æœå‹™")
    await stop_scheduler()

app = FastAPI(
    title="æ–°è API with Scheduler",
    description="æä¾›æ–°èæ•¸æ“šçš„ RESTful APIï¼Œå…§å»ºè‡ªå‹•çˆ¬èŸ²æ’ç¨‹",
    version="2.0.0",
    lifespan=lifespan
)

# è³‡æ–™åº«é…ç½®
def get_database_url():
    """ç²å–è³‡æ–™åº«é€£ç·š URL"""
    database_type = os.getenv('DATABASE_TYPE', 'sqlite')
    
    if database_type == 'postgresql':
        host = os.getenv('POSTGRES_HOST', 'localhost')
        port = os.getenv('POSTGRES_PORT', '5432')
        database = os.getenv('POSTGRES_DATABASE', 'news_analyze')
        user = os.getenv('POSTGRES_USER', 'news_user')
        password = os.getenv('POSTGRES_PASSWORD', 'news_password_2024')
        return f'postgresql://{user}:{password}@{host}:{port}/{database}'
    else:
        sqlite_path = os.getenv('SQLITE_DB_PATH', './news.db')
        return f'sqlite:///{sqlite_path}'

# å»ºç«‹è³‡æ–™åº«é€£ç·š
engine = create_engine(get_database_url())
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# ä¾è³´æ³¨å…¥ï¼šç²å–è³‡æ–™åº«æœƒè©±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Pydantic æ¨¡å‹
class NewsResponse(BaseModel):
    id: int
    title: str
    content: Optional[str] = None
    url: Optional[str] = None
    source: str
    publish_time: Optional[datetime] = None
    created_at: datetime
    updated_at: Optional[datetime] = None

class NewsListResponse(BaseModel):
    total: int
    page: int
    per_page: int
    pages: int
    data: List[NewsResponse]

class StatsResponse(BaseModel):
    total_news: int
    sources: Dict[str, int]
    latest_update: Optional[datetime] = None

class SchedulerStatusResponse(BaseModel):
    status: str
    interval_hours: int
    next_run: Optional[datetime] = None
    last_run: Optional[datetime] = None

# API è·¯ç”±
@app.get("/")
async def root():
    """æ ¹è·¯ç”±"""
    interval = get_scheduler_interval()
    return {
        "message": "æ–°è API æœå‹™å™¨ with Scheduler",
        "version": "2.0.0",
        "docs": "/docs",
        "scheduler": {
            "enabled": True,
            "interval_hours": interval
        }
    }

@app.get("/health")
async def health_check(db: Session = Depends(get_db)):
    """å¥åº·æª¢æŸ¥"""
    try:
        # æ¸¬è©¦è³‡æ–™åº«é€£ç·š
        result = db.execute(text("SELECT 1"))
        result.fetchone()
        
        # ç²å–æ’ç¨‹å™¨ç‹€æ…‹
        scheduler_status = get_scheduler_status()
        
        return {
            "status": "healthy",
            "database": "connected",
            "scheduler": scheduler_status["status"],
            "timestamp": datetime.now()
        }
    except Exception as e:
        logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        raise HTTPException(status_code=503, detail="æœå‹™æš«æ™‚ä¸å¯ç”¨")

@app.post("/scraper/run")
async def run_scraper_manually(background_tasks: BackgroundTasks):
    """æ‰‹å‹•åŸ·è¡Œçˆ¬èŸ²"""
    try:
        background_tasks.add_task(run_scraper_job)
        logger.info("ğŸ”„ æ‰‹å‹•çˆ¬èŸ²ä»»å‹™å·²åŠ å…¥èƒŒæ™¯åŸ·è¡Œ")
        
        return {
            "message": "çˆ¬èŸ²ä»»å‹™å·²é–‹å§‹åŸ·è¡Œ",
            "status": "running",
            "timestamp": datetime.now()
        }
    except Exception as e:
        logger.error(f"æ‰‹å‹•åŸ·è¡Œçˆ¬èŸ²å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="ç„¡æ³•åŸ·è¡Œçˆ¬èŸ²ä»»å‹™")

@app.get("/scheduler/status")
async def get_scheduler_status_api():
    """ç²å–æ’ç¨‹å™¨ç‹€æ…‹"""
    try:
        status_info = get_scheduler_status()
        interval = get_scheduler_interval()
        
        # è¨ˆç®—ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“
        now = datetime.now()
        if interval == 24:
            next_run = now.replace(hour=2, minute=0, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=1)
        else:
            next_run = now + timedelta(hours=interval)
        
        return {
            "status": status_info["status"],
            "interval_hours": interval,
            "next_run": next_run,
            "last_run": None  # TODO: å¯¦ä½œä¸Šæ¬¡åŸ·è¡Œæ™‚é–“è¿½è¹¤
        }
    except Exception as e:
        logger.error(f"ç²å–æ’ç¨‹å™¨ç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="ç„¡æ³•ç²å–æ’ç¨‹å™¨ç‹€æ…‹")

@app.get("/news", response_model=NewsListResponse)
async def get_news(
    page: int = Query(1, ge=1, description="é ç¢¼"),
    per_page: int = Query(20, ge=1, le=100, description="æ¯é æ•¸é‡"),
    source: Optional[str] = Query(None, description="æ–°èä¾†æºéæ¿¾"),
    search: Optional[str] = Query(None, description="æ¨™é¡Œæœå°‹é—œéµå­—"),
    start_date: Optional[str] = Query(None, description="é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
    db: Session = Depends(get_db)
):
    """ç²å–æ–°èåˆ—è¡¨"""
    try:
        # å»ºæ§‹æŸ¥è©¢æ¢ä»¶
        conditions = []
        params = {}
        
        if source:
            conditions.append("source = :source")
            params['source'] = source
        
        if search:
            conditions.append("title LIKE :search")
            params['search'] = f"%{search}%"
        
        if start_date:
            conditions.append("DATE(created_at) >= :start_date")
            params['start_date'] = start_date
        
        if end_date:
            conditions.append("DATE(created_at) <= :end_date")
            params['end_date'] = end_date
        
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        
        # è¨ˆç®—ç¸½æ•¸
        count_query = f"SELECT COUNT(*) FROM news WHERE {where_clause}"
        total_result = db.execute(text(count_query), params)
        total = total_result.scalar() or 0
        
        # è¨ˆç®—åˆ†é 
        offset = (page - 1) * per_page
        pages = (total + per_page - 1) // per_page
        
        # ç²å–æ•¸æ“š
        data_query = f"""
            SELECT id, title, content, url, source, publish_time, created_at, updated_at
            FROM news 
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT :limit OFFSET :offset
        """
        params.update({'limit': per_page, 'offset': offset})
        
        result = db.execute(text(data_query), params)
        rows = result.fetchall()
        
        # è½‰æ›ç‚ºéŸ¿æ‡‰æ¨¡å‹
        news_data = []
        for row in rows:
            news_item = NewsResponse(
                id=row[0],
                title=row[1] or "",
                content=row[2],
                url=row[3],
                source=row[4] or "",
                publish_time=row[5],
                created_at=row[6],
                updated_at=row[7]
            )
            news_data.append(news_item)
        
        return NewsListResponse(
            total=total,
            page=page,
            per_page=per_page,
            pages=pages,
            data=news_data
        )
        
    except Exception as e:
        logger.error(f"ç²å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="ç²å–æ–°èåˆ—è¡¨å¤±æ•—")

@app.get("/stats", response_model=StatsResponse)
async def get_stats(db: Session = Depends(get_db)):
    """ç²å–çµ±è¨ˆä¿¡æ¯"""
    try:
        # ç¸½æ–°èæ•¸
        total_query = "SELECT COUNT(*) FROM news"
        total_result = db.execute(text(total_query))
        total_news = total_result.scalar() or 0
        
        # å„ä¾†æºçµ±è¨ˆ
        sources_query = "SELECT source, COUNT(*) FROM news GROUP BY source"
        sources_result = db.execute(text(sources_query))
        sources = {row[0]: row[1] for row in sources_result.fetchall()}
        
        # æœ€æ–°æ›´æ–°æ™‚é–“
        latest_query = "SELECT MAX(created_at) FROM news"
        latest_result = db.execute(text(latest_query))
        latest_update = latest_result.scalar()
        
        return StatsResponse(
            total_news=total_news,
            sources=sources,
            latest_update=latest_update
        )
        
    except Exception as e:
        logger.error(f"ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—")

@app.get("/news/recent")
async def get_recent_news(
    limit: int = Query(10, ge=1, le=50, description="è¿”å›æ•¸é‡"),
    source: Optional[str] = Query(None, description="æ–°èä¾†æºéæ¿¾"),
    db: Session = Depends(get_db)
):
    """ç²å–æœ€æ–°æ–°è"""
    try:
        conditions = []
        params = {'limit': limit}
        
        if source:
            conditions.append("source = :source")
            params['source'] = source
        
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        
        query = f"""
            SELECT id, title, url, source, publish_time, created_at
            FROM news 
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT :limit
        """
        
        result = db.execute(text(query), params)
        rows = result.fetchall()
        
        news_data = []
        for row in rows:
            news_data.append({
                "id": row[0],
                "title": row[1],
                "url": row[2],
                "source": row[3],
                "publish_time": row[4],
                "created_at": row[5]
            })
        
        return {
            "count": len(news_data),
            "data": news_data
        }
        
    except Exception as e:
        logger.error(f"ç²å–æœ€æ–°æ–°èå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="ç²å–æœ€æ–°æ–°èå¤±æ•—")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
