#!/usr/bin/env python3
"""
æ–°èçˆ¬èŸ²çµ±ä¸€ç®¡ç†å™¨ - ä½¿ç”¨ORMç‰ˆæœ¬
"""

import sys
import os
from datetime import datetime
from typing import Dict, Any
import concurrent.futures
import threading

# å°å…¥ORMç‰ˆæœ¬çš„çˆ¬èŸ²
try:
    from scrapying.setn_new import SETNScraper
except ImportError:
    SETNScraper = None
    print("è­¦å‘Š: SETNçˆ¬èŸ²å°å…¥å¤±æ•—")

try:
    from scrapying.ltn_scraper_orm import LTNScraper  
except ImportError:
    LTNScraper = None
    print("è­¦å‘Š: LTNçˆ¬èŸ²å°å…¥å¤±æ•—")

try:
    from scrapying.tvbs_scraper_orm import TVBSScraper
except ImportError:
    TVBSScraper = None
    print("è­¦å‘Š: TVBSçˆ¬èŸ²å°å…¥å¤±æ•—")

try:
    from scrapying.chinatimes_scraper_orm import ChinaTimesScraper
except ImportError:
    ChinaTimesScraper = None
    print("è­¦å‘Š: ChinaTimesçˆ¬èŸ²å°å…¥å¤±æ•—")

from news_orm_db import NewsORMDatabase

news_orm_db = NewsORMDatabase()


class UnifiedScraperManager:
    """çµ±ä¸€çš„æ–°èçˆ¬èŸ²ç®¡ç†å™¨ - ORMç‰ˆæœ¬"""
    
    def __init__(self):
        self.scrapers = {}
        self._initialize_scrapers()
    
    def _initialize_scrapers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„çˆ¬èŸ²"""
        if SETNScraper:
            self.scrapers['SETN'] = SETNScraper()
        
        if LTNScraper:
            self.scrapers['LTN'] = LTNScraper()
        
        if TVBSScraper:
            self.scrapers['TVBS'] = TVBSScraper()
        
        if ChinaTimesScraper:
            self.scrapers['ChinaTimes'] = ChinaTimesScraper()
        
        print(f"å·²åˆå§‹åŒ– {len(self.scrapers)} å€‹çˆ¬èŸ²: {list(self.scrapers.keys())}")
    
    def _scrape_single_source(self, name: str, scraper, max_pages: int = 1) -> tuple:
        """å–®ä¸€çˆ¬èŸ²åŸ·è¡Œå‡½æ•¸"""
        try:
            print(f"ğŸ”„ é–‹å§‹åŸ·è¡Œ {name} çˆ¬èŸ²...")
            result = scraper.scrape_news(max_pages=max_pages)
            print(f"âœ… {name} å®Œæˆ - ç¸½è¨ˆ:{result['total']}, æ–°å¢:{result['new']}, è·³é:{result['skipped']}, å¤±æ•—:{result['failed']}")
            return name, result
        except Exception as e:
            print(f"âŒ {name} çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            return name, {'error': str(e)}
    
    def run_all_scrapers(self, max_pages: int = 1) -> Dict[str, Any]:
        """ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²"""
        print(f"ğŸš€ ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰çˆ¬èŸ² (æ¯å€‹çˆ¬èŸ²æœ€å¤š {max_pages} é )")
        print("=" * 50)
        
        results = {}
        total_stats = {'total': 0, 'new': 0, 'skipped': 0, 'failed': 0}
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.scrapers)) as executor:
            # æäº¤æ‰€æœ‰çˆ¬èŸ²ä»»å‹™
            futures = {
                executor.submit(self._scrape_single_source, name, scraper, max_pages): name
                for name, scraper in self.scrapers.items()
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                name, result = future.result()
                results[name] = result
                
                # ç´¯è¨ˆçµ±è¨ˆï¼ˆåªæœ‰æˆåŠŸçš„çµæœæ‰è¨ˆå…¥ï¼‰
                if 'error' not in result:
                    for key in total_stats:
                        if key in result and isinstance(result[key], int):
                            total_stats[key] += result[key]
        
        results['ç¸½è¨ˆ'] = total_stats
        print(f"\nğŸ“Š ä¸¦è¡ŒåŸ·è¡Œå®Œæˆï¼ç¸½è¨ˆ: {total_stats}")
        return results
    
    def run_single_scraper(self, scraper_name: str, max_pages: int = 1) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹çˆ¬èŸ²"""
        if scraper_name not in self.scrapers:
            available = list(self.scrapers.keys())
            raise ValueError(f"çˆ¬èŸ² '{scraper_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨çš„çˆ¬èŸ²: {available}")
        
        print(f"åŸ·è¡Œ {scraper_name} çˆ¬èŸ² (æœ€å¤š {max_pages} é )")
        scraper = self.scrapers[scraper_name]
        return scraper.scrape_news(max_pages=max_pages)
    
    def get_database_stats(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯"""
        print("è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯:")
        print("-" * 30)
        
        total_count = news_orm_db.get_news_count()
        print(f"ç¸½æ–°èæ•¸: {total_count}")
        
        source_counts = news_orm_db.get_news_count_by_source()
        print("\nå„ä¾†æºçµ±è¨ˆ:")
        for source, count in source_counts:
            print(f"  {source}: {count} æ¢æ–°è")
        
        return {
            'total_count': total_count,
            'source_counts': dict(source_counts)
        }
    
    def show_recent_news(self, limit: int = 10):
        """é¡¯ç¤ºæœ€è¿‘çš„æ–°è"""
        print(f"\næœ€è¿‘ {limit} æ¢æ–°è:")
        print("-" * 50)
        
        recent_news = news_orm_db.get_recent_news(limit)
        for i, news in enumerate(recent_news, 1):
            print(f"{i:2d}. [{news['news_source']}] {news['title'][:60]}...")
            print(f"     ä½œè€…: {news['author']} | æ™‚é–“: {news['publish_time']}")


def main():
    """ä¸»å‡½æ•¸"""
    print("æ–°èçˆ¬èŸ²çµ±ä¸€ç®¡ç†ç³»çµ± (ORMç‰ˆæœ¬)")
    print("=" * 50)
    
    manager = UnifiedScraperManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == 'all':
            # åŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²
            max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 1
            results = manager.run_all_scrapers(max_pages)
            
        elif command in ['setn', 'ltn', 'tvbs', 'chinatimes']:
            # åŸ·è¡ŒæŒ‡å®šçˆ¬èŸ²
            max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 1
            scraper_name = command.upper()
            if scraper_name == 'CHINATIMES':
                scraper_name = 'ChinaTimes'
            result = manager.run_single_scraper(scraper_name, max_pages)
            print(f"åŸ·è¡Œçµæœ: {result}")
            
        elif command == 'stats':
            # é¡¯ç¤ºçµ±è¨ˆ
            manager.get_database_stats()
            manager.show_recent_news()
            return
            
        else:
            print("ç„¡æ•ˆçš„å‘½ä»¤")
            print("ä½¿ç”¨æ–¹å¼:")
            print("  python unified_manager_orm.py all [é æ•¸]     # åŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²")
            print("  python unified_manager_orm.py setn [é æ•¸]    # åŸ·è¡ŒSETNçˆ¬èŸ²")
            print("  python unified_manager_orm.py ltn [é æ•¸]     # åŸ·è¡ŒLTNçˆ¬èŸ²")
            print("  python unified_manager_orm.py tvbs [é æ•¸]    # åŸ·è¡ŒTVBSçˆ¬èŸ²")
            print("  python unified_manager_orm.py chinatimes [é æ•¸]  # åŸ·è¡Œä¸­åœ‹æ™‚å ±çˆ¬èŸ²")
            print("  python unified_manager_orm.py stats         # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯")
            return
    else:
        # é»˜èªåŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²ï¼ˆ1é ï¼‰
        results = manager.run_all_scrapers(1)
    
    # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 50)
    print("æœ€çµ‚çµ±è¨ˆ:")
    manager.get_database_stats()
    manager.show_recent_news(5)


if __name__ == "__main__":
    main()
